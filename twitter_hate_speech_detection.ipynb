{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb99a163",
   "metadata": {},
   "source": [
    "# Project: - Twitter Hate Speech Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6559eb",
   "metadata": {},
   "source": [
    "# Name: - Siddhant Agrawal\n",
    "\n",
    "# Roll no:- C201\n",
    "\n",
    "# Class:- B.tech AIML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036f9a9b",
   "metadata": {},
   "source": [
    "## Importing all required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee90c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c7f8344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SIDDHANT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SIDDHANT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb32e20",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e4b8c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7baf97a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9017551f",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da288987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(text):\n",
    "    #removing mentions or any urls\n",
    "    text = re.sub(r'@[\\w]*', '', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    \n",
    "    # lowercasing and removing punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # removing stopwords\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5702a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_tweet'] = data['tweet'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38fcf5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('cleaned_twitter_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9934e839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       cleaned_tweet\n",
      "0  father dysfunctional selfish drags kids dysfun...\n",
      "1  thanks lyft credit cant use cause dont offer w...\n",
      "2                                     bihday majesty\n",
      "3  model love u take u time urð± ðððð...\n",
      "4                      factsguide society motivation\n"
     ]
    }
   ],
   "source": [
    "print(data[['cleaned_tweet']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f2e779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3173b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['cleaned_tweet'] = test_data['tweet'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05764acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c024769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = data['cleaned_tweet'] \n",
    "test_tweets = test_data['cleaned_tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c501f20",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21dc241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "784ad111",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tfidf_vectorizer.fit_transform(train_tweets)\n",
    "X_test = tfidf_vectorizer.transform(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26d788ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b91bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dense = X_train.toarray()\n",
    "X_test_dense = X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10f5934d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (31962, 5000)\n",
      "Shape of X_test: (17197, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f24dbdf",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd6e9d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fe2342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b557262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SIDDHANT\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model.add(Input(shape=(X_train.shape[1],)))\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1f35768",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d60dbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m899/899\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - accuracy: 0.9352 - loss: 0.2397 - val_accuracy: 0.9581 - val_loss: 0.1281\n",
      "Epoch 2/5\n",
      "\u001b[1m899/899\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.9718 - loss: 0.0770 - val_accuracy: 0.9571 - val_loss: 0.1365\n",
      "Epoch 3/5\n",
      "\u001b[1m899/899\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.9841 - loss: 0.0489 - val_accuracy: 0.9550 - val_loss: 0.1656\n",
      "Epoch 4/5\n",
      "\u001b[1m899/899\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.9914 - loss: 0.0270 - val_accuracy: 0.9556 - val_loss: 0.1947\n",
      "Epoch 5/5\n",
      "\u001b[1m899/899\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.9957 - loss: 0.0144 - val_accuracy: 0.9537 - val_loss: 0.2461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1843eb941d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_dense, y_train, epochs=5, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5568daaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m538/538\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = (model.predict(X_test_dense) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3b3609",
   "metadata": {},
   "source": [
    "## Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e529acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hate_speech(text):\n",
    "    # Step 1: Clean the text (using your previously defined cleaning function)\n",
    "    cleaned_text = clean_tweet(text)  # Make sure this function is defined in your code\n",
    "\n",
    "    # Step 2: Transform the cleaned text to TF-IDF features\n",
    "    text_tfidf = tfidf_vectorizer.transform([cleaned_text]).toarray()  # Wrap in a list for single input\n",
    "\n",
    "    # Step 3: Predict using the trained model\n",
    "    prediction = model.predict(text_tfidf)\n",
    "\n",
    "    # Step 4: Return the predicted label (1 or 0)\n",
    "    return (prediction > 0.5).astype(int)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "704086ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Not a Hate Speech.\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"thanks for helping me\"\n",
    "predicted_label = predict_hate_speech(sample_text)\n",
    "if(predicted_label==0):\n",
    "    print(\"Not a Hate Speech.\")\n",
    "else:\n",
    "    print(\"Hate Speech!!!\")\n",
    "#print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b737fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
